import os
import json
import hashlib
from datetime import datetime
from pathlib import Path
from mitmproxy import ctx

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    "output_directory": "MetaTrader_Brokers_Data",  # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
    "url_filter": "updates.metaquotes.net/public/mt5/network/mobile",
    "save_duplicates": False,  # –ò–∑–±–µ–≥–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    "add_timestamp": True,     # –î–æ–±–∞–≤–ª—è—Ç—å –≤—Ä–µ–º—è –≤ –∏–º—è —Ñ–∞–π–ª–∞
    "detailed_logging": True   # –ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
BASE_DIR = Path(__file__).parent / CONFIG["output_directory"]
response_counter = 1
processed_hashes = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

def ensure_directory_exists():
    """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
    try:
        BASE_DIR.mkdir(parents=True, exist_ok=True)
        return True
    except Exception as e:
        ctx.log.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {BASE_DIR}: {e}")
        return False

def get_content_hash(content):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö—ç—à –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def generate_filename(counter, add_timestamp=True):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S") if add_timestamp else ""
    if timestamp:
        return f"response_{counter}_{timestamp}.json"
    return f"response_{counter}.json"

def response(flow):
    global response_counter, processed_hashes
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ URL
    if CONFIG["url_filter"] in flow.request.url:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_type = flow.response.headers.get("content-type", "")
        if "application/json" in content_type:
            try:
                # –ü–∞—Ä—Å–∏–Ω–≥ JSON –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                response_content = flow.response.content.decode('utf-8')
                json_data = json.loads(response_content)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
                if not CONFIG["save_duplicates"]:
                    content_hash = get_content_hash(response_content)
                    if content_hash in processed_hashes:
                        if CONFIG["detailed_logging"]:
                            ctx.log.info(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç –æ—Ç–≤–µ—Ç–∞ (hash: {content_hash[:8]}...)")
                        return
                    processed_hashes.add(content_hash)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                if not ensure_directory_exists():
                    return
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                filename = generate_filename(response_counter, CONFIG["add_timestamp"])
                filepath = BASE_DIR / filename
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON —Å –∫—Ä–∞—Å–∏–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
                with open(filepath, "w", encoding='utf-8') as f:
                    json.dump(json_data, f, indent=4, ensure_ascii=False)
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                file_size = filepath.stat().st_size
                if CONFIG["detailed_logging"]:
                    ctx.log.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω –æ—Ç–≤–µ—Ç #{response_counter}: {filename} ({file_size} –±–∞–π—Ç)")
                    ctx.log.info(f"üìÅ –ü—É—Ç—å: {filepath}")
                    if "brokers" in json_data:
                        broker_count = len(json_data.get("brokers", []))
                        ctx.log.info(f"üè¢ –ù–∞–π–¥–µ–Ω–æ –±—Ä–æ–∫–µ—Ä–æ–≤: {broker_count}")
                else:
                    ctx.log.info(f"JSON –æ—Ç–≤–µ—Ç {response_counter} —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
                
                response_counter += 1
                
            except json.JSONDecodeError as e:
                ctx.log.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON: {e}")
            except UnicodeDecodeError as e:
                ctx.log.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏: {e}")
            except IOError as e:
                ctx.log.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}")
            except Exception as e:
                ctx.log.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–∫—Ä–∏–ø—Ç–∞
if ensure_directory_exists():
    ctx.log.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Global_JSON_Response_Extractor")
    ctx.log.info(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {BASE_DIR.absolute()}")
    ctx.log.info(f"üéØ –§–∏–ª—å—Ç—Ä URL: {CONFIG['url_filter']}")
    ctx.log.info(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç—ã: {'—Ä–∞–∑—Ä–µ—à–µ–Ω—ã' if CONFIG['save_duplicates'] else '–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã'}")
else:
    ctx.log.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä")

